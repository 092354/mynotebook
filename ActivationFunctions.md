# Activation Functions

- Linear Regression
  > z = wx + b
  > 
  > g(z) = z
- ReLU (Rectified Linear Unit)
  > z = wx + b
  > 
  > g(z) = max(0,z)
- Sigmoid
  > z = wx + b
  > 
  > $g(z) =1/(1+ e^{-z})$ 
- Softmax
